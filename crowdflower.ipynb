{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv').fillna('')\n",
    "test = pd.read_csv('test.csv').fillna('')\n",
    "\n",
    "cols = ['query', 'product_title', 'product_description']\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "pairs = list(combinations(cols, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Apply lowercasing and stemming for each string in the `query`, `product_title`, and `product_description` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "token_pattern = re.compile(r'(?u)\\b\\w\\w+\\b')\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = [token.lower() for token in token_pattern.findall(text)]\n",
    "    return ' '.join([stemmer.stem(token) for token in tokens if token not in stopwords])\n",
    "\n",
    "def preprocess_df(df):    \n",
    "    for col in cols:\n",
    "        df[col] = df[col].map(preprocess)\n",
    "\n",
    "preprocess_df(train)\n",
    "preprocess_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate $n$-grams, $n \\in \\left\\{1, 2, 3\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from functools import partial\n",
    "\n",
    "def ngramize(col, n, row):\n",
    "    # I join the ngrams to avoid w1w2w3 unigrams becoming [(w1,), (w2,), (w3,)]\n",
    "    return [' '.join(ngram) for ngram in ngrams(row[col].split(), n)]\n",
    "\n",
    "def generate_ngrams(df):\n",
    "    for col in cols:\n",
    "        df['%s_%s' % (col, 'unigram')] = df.apply(partial(ngramize, col, 1), axis='columns')\n",
    "        df['%s_%s' % (col, 'bigram')] = df.apply(partial(ngramize, col, 2), axis='columns')\n",
    "        df['%s_%s' % (col, 'trigram')] = df.apply(partial(ngramize, col, 3), axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Features\n",
    "\n",
    "Following some pointers from Owen Zhang (currently ranked 6th in Kaggle), I'm extracting \"trivial\" features such as the length of text, and number of $n$-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_counting_features(df):\n",
    "    for col in cols:\n",
    "        for gram in ['unigram', 'bigram', 'trigram']:\n",
    "            counted = '%s_%s' % (col, gram)\n",
    "            df['count_%s' % counted] = df.apply(lambda row: len(row[counted]), axis='columns')\n",
    "            df['count_dist_%s' % counted] = df.apply(lambda row: len(set(row[counted])), axis='columns')\n",
    "            df['ratio_%s' % counted] = df['count_dist_%s' % counted] / df['count_%s' % counted]\n",
    "            df['ratio_%s' % counted] = df['ratio_%s' % counted].fillna(0)  # in case of division by zero, above\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Features\n",
    "\n",
    "Following some Wikipedia-ing and asking around the halls of building 37, there are two distance metrics which are equivalent, yet not identical: [Jaccard](https://en.wikipedia.org/wiki/Jaccard_index) and [Dice](https://en.wikipedia.org/wiki/Dice's coefficient). They measure how similar two sets are, but this also works for $n$-grams, as shown in the Dice coefficient Wiki page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def jaccard(a, b):\n",
    "    A, B = set(a), set(b)\n",
    "    try:\n",
    "        return len(A & B) / len(A | B)\n",
    "    except ZeroDivisionError:\n",
    "        return 0.\n",
    "\n",
    "def dice(a, b):\n",
    "    A, B = set(a), set(b)\n",
    "    try:\n",
    "        return 2 * len(A & B) / (len(A) + len(B))\n",
    "    except ZeroDivisionError:\n",
    "        return 0.\n",
    "\n",
    "def extract_distance_features(df):\n",
    "    for f in [jaccard, dice]:\n",
    "        for gram in ['unigram', 'bigram', 'trigram']:\n",
    "            for p in pairs:\n",
    "                df['%s_%s_%s_%s' % (f.__name__, gram, p[0], p[1])] = df.apply(lambda row: f(row['%s_%s' % (p[0], gram)],\n",
    "                                                                                            row['%s_%s' % (p[1], gram)]),\n",
    "                                                                              axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Features\n",
    "\n",
    "After some head-banging, I figured out that first, I need to `fit` my vectorizer on the combination of `query`, `product_title`, and `product_description`. This way, the vectorizer knows every possible word, and I can use cosine similarity between the `transform`-ed pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extract_tfidf_features(df):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    vectorizer.fit(df[cols[0]] + ' ' + df[cols[1]] + ' ' + df[cols[2]])\n",
    "    for p in pairs:\n",
    "        x = vectorizer.transform(df[p[0]])\n",
    "        y = vectorizer.transform(df[p[1]])\n",
    "        df['%s_%s_cosim' % p] = cosine_similarity(x, y)[0][0] or 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Features\n",
    "\n",
    "The following code is taken from the Python benchmark on Kaggle by Ben Hammer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "class FeatureMapper:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for column_name, extractor in self.features:\n",
    "            extractor.fit(X[column_name], y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        extracted = []\n",
    "        for column_name, extractor in self.features:\n",
    "            fea = extractor.transform(X[column_name])\n",
    "            if hasattr(fea, \"toarray\"):\n",
    "                extracted.append(fea.toarray())\n",
    "            else:\n",
    "                extracted.append(fea)\n",
    "        if len(extracted) > 1:\n",
    "            return np.concatenate(extracted, axis=1)\n",
    "        else: \n",
    "            return extracted[0]\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        extracted = []\n",
    "        for column_name, extractor in self.features:\n",
    "            fea = extractor.fit_transform(X[column_name], y)\n",
    "            if hasattr(fea, \"toarray\"):\n",
    "                extracted.append(fea.toarray())\n",
    "            else:\n",
    "                extracted.append(fea)\n",
    "        if len(extracted) > 1:\n",
    "            return np.concatenate(extracted, axis=1)\n",
    "        else: \n",
    "            return extracted[0]\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "class SimpleTransform(BaseEstimator):\n",
    "    def __init__(self, transformer=identity):\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([self.transformer(x) for x in X], ndmin=2).T\n",
    "\n",
    "features = FeatureMapper([('count_query_unigram', SimpleTransform()),\n",
    "                          ('count_dist_query_unigram', SimpleTransform()),\n",
    "                          ('ratio_query_unigram', SimpleTransform()),\n",
    "                          ('count_query_bigram', SimpleTransform()),\n",
    "                          ('count_dist_query_bigram', SimpleTransform()),\n",
    "                          ('ratio_query_bigram', SimpleTransform()),\n",
    "                          ('count_query_trigram', SimpleTransform()),\n",
    "                          ('count_dist_query_trigram', SimpleTransform()),\n",
    "                          ('ratio_query_trigram', SimpleTransform()),\n",
    "                          ('count_product_title_unigram', SimpleTransform()),\n",
    "                          ('count_dist_product_title_unigram', SimpleTransform()),\n",
    "                          ('ratio_product_title_unigram', SimpleTransform()),\n",
    "                          ('count_product_title_bigram', SimpleTransform()),\n",
    "                          ('count_dist_product_title_bigram', SimpleTransform()),\n",
    "                          ('ratio_product_title_bigram', SimpleTransform()),\n",
    "                          ('count_product_title_trigram', SimpleTransform()),\n",
    "                          ('count_dist_product_title_trigram', SimpleTransform()),\n",
    "                          ('ratio_product_title_trigram', SimpleTransform()),\n",
    "                          ('count_product_description_unigram', SimpleTransform()),\n",
    "                          ('count_dist_product_description_unigram', SimpleTransform()),\n",
    "                          ('ratio_product_description_unigram', SimpleTransform()),\n",
    "                          ('count_product_description_bigram', SimpleTransform()),\n",
    "                          ('count_dist_product_description_bigram', SimpleTransform()),\n",
    "                          ('ratio_product_description_bigram', SimpleTransform()),\n",
    "                          ('count_product_description_trigram', SimpleTransform()),\n",
    "                          ('count_dist_product_description_trigram', SimpleTransform()),\n",
    "                          ('ratio_product_description_trigram', SimpleTransform()),\n",
    "                          ('jaccard_unigram_query_product_title', SimpleTransform()),\n",
    "                          ('jaccard_unigram_query_product_description', SimpleTransform()),\n",
    "                          ('jaccard_unigram_product_title_product_description', SimpleTransform()),\n",
    "                          ('jaccard_bigram_query_product_title', SimpleTransform()),\n",
    "                          ('jaccard_bigram_query_product_description', SimpleTransform()),\n",
    "                          ('jaccard_bigram_product_title_product_description', SimpleTransform()),\n",
    "                          ('jaccard_trigram_query_product_title', SimpleTransform()),\n",
    "                          ('jaccard_trigram_query_product_description', SimpleTransform()),\n",
    "                          ('jaccard_trigram_product_title_product_description', SimpleTransform()),\n",
    "                          ('dice_unigram_query_product_title', SimpleTransform()),\n",
    "                          ('dice_unigram_query_product_description', SimpleTransform()),\n",
    "                          ('dice_unigram_product_title_product_description', SimpleTransform()),\n",
    "                          ('dice_bigram_query_product_title', SimpleTransform()),\n",
    "                          ('dice_bigram_query_product_description', SimpleTransform()),\n",
    "                          ('dice_bigram_product_title_product_description', SimpleTransform()),\n",
    "                          ('dice_trigram_query_product_title', SimpleTransform()),\n",
    "                          ('dice_trigram_query_product_description', SimpleTransform()),\n",
    "                          ('dice_trigram_product_title_product_description', SimpleTransform()),\n",
    "                          ('query_product_title_cosim', SimpleTransform()),\n",
    "                          ('query_product_description_cosim', SimpleTransform()),\n",
    "                          ('product_title_product_description_cosim', SimpleTransform())])\n",
    "\n",
    "def extract_features(df):\n",
    "    generate_ngrams(df)\n",
    "    extract_counting_features(df)\n",
    "    extract_distance_features(df)\n",
    "    extract_tfidf_features(df)\n",
    "\n",
    "extract_features(train)\n",
    "extract_features(test)\n",
    "\n",
    "pipeline = Pipeline([(\"extract_features\", features),\n",
    "                     (\"regress\", ExtraTreesRegressor(n_estimators=100,n_jobs=-1))])\n",
    "\n",
    "pipeline.fit(train, train[\"median_relevance\"])\n",
    "\n",
    "predictions = pipeline.predict(test).round().astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\"id\": test[\"id\"], \"prediction\": predictions})\n",
    "submission.to_csv(\"ben_daniel_ex4.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
